
# Challenge 3: Data Engineering con PySpark

Este proyecto es parte de un conjunto de retos semanales enfocados en mejorar habilidades de Data Engineering utilizando PySpark. En esta tercera semana, el objetivo es realizar diversas transformaciones y anÃ¡lisis sobre datos estructurados en formato CSV y JSON.

## ğŸš€ Objetivos del Proyecto

1. Leer y procesar datos desde archivos JSON y CSV.
2. Realizar conversiones de tipos de datos, como `timestamp`, `float` e `integer`.
3. Aplicar agregaciones y operaciones sobre los datos utilizando PySpark.

---

## ğŸ› ï¸ Requisitos

- **Python** 3.8 o superior.
- **PySpark** 3.x.
- **VS Code** (opcional, para desarrollo).
- **Data Files**:  
  - `clientes_reducido.csv`
  - `transacciones_correcto.json`

---

## ğŸ“‚ Estructura del Proyecto

```plaintext
â”œâ”€â”€ resources/
â”‚   â”œâ”€â”€ clientes_reducido.csv        # Datos de clientes
â”‚   â”œâ”€â”€ transacciones_correcto.json  # Transacciones en formato JSON
â”œâ”€â”€ main.py                          # CÃ³digo principal del reto
â”œâ”€â”€ README.md                        # DocumentaciÃ³n del proyecto
â”œâ”€â”€ libraries/
â”‚   â”œâ”€â”€ util.py                      # MÃ©todos utiles
â”‚   â”œâ”€â”€ pyspark_env.py               # LÃ³gica en version PySpark
â”‚   â”œâ”€â”€ sparksql_env.py              # LÃ³gica en version SparkSql
```

---

## ğŸ“¦ InstalaciÃ³n

1. **Clona el repositorio**:

   ```bash
   git clone https://github.com/CrixusVolcanic/Challenge-3-PySpark.git
   cd Challenge-3-PySpark
   ```

2. **Instala las dependencias**:

   ```bash
   pip install -r requirements.txt
   ```

3. **Verifica que PySpark estÃ© instalado**:

   ```bash
   pyspark --version
   ```

---

## ğŸ”§ Uso

1. Coloca los archivos `clientes_reducido.csv` y `transacciones_correcto.json` en la carpeta `resources/`.

2. Ejecuta el archivo principal:

   ```bash
   python main.py
   ```

3. **Salida esperada**:
   - Un esquema limpio de datos con los tipos convertidos.
   - Resultados de agrupaciones y transformaciones.

---

## âœ¨ Funciones Clave

### Lectura de Datos
```python
df = spark.read.format("json").load("resources/transacciones_correcto.json")
```

### Transformaciones de Datos
- Cambiar tipos de datos (`cast`).
- Convertir cadenas en formato de fecha a tipo `timestamp`.

```python
df = df.withColumn("amount", df["amount"].cast(FloatType())) \
       .withColumn("timestamp", to_timestamp(df["timestamp"]))
```

### Agregaciones
- Contar transacciones por moneda:
```python
df.groupBy("currency").count().show()
```

---

## ğŸ“ Notas

- AsegÃºrate de que los archivos estÃ©n en formato esperado. 
  - El JSON debe estar en formato de lÃ­neas (`.jsonl`) para una correcta lectura.
- Si tienes problemas con los atajos de teclado en VS Code, revisa las configuraciones de tu terminal.

---

## ğŸ“š Recursos Adicionales

- [DocumentaciÃ³n oficial de PySpark](https://spark.apache.org/docs/latest/api/python/)
- [Atajos de teclado de VS Code](https://code.visualstudio.com/shortcuts/keyboard-shortcuts-macos.pdf)

---

## ğŸ§‘â€ğŸ’» Autor

- **Davian Bermudez**  
  Data Engineer apasionada por aprender y crecer en el ecosistema de Big Data.

---

## ğŸ¯ PrÃ³ximos Pasos

- Mejorar la gestiÃ³n de errores al leer datos.
- AÃ±adir mÃ¡s anÃ¡lisis como funciones de ventana y joins avanzados.
